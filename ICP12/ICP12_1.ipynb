{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ICP12-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ba-FjJfowc0",
        "outputId": "aa738b02-ab39-4bca-dee4-db578b560c03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# loading libraries \n",
        "import sys\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Embedding, Dropout, LSTM, SpatialDropout1D\n",
        "import re\n",
        "from keras.utils import np_utils\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import numpy\n",
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load the provided CSV file “Sentiment.csv” and process this file as needed to handle text data."
      ],
      "metadata": {
        "id": "xvOlUM0OrBlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading dataset\n",
        "dataset = pd.read_csv('/content/Sentiment.csv')\n",
        "dataset = dataset[['text','sentiment']]\n",
        "dataset['text'] = dataset['text'].apply(lambda x: x.lower())\n",
        "dataset['text'] = dataset['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))"
      ],
      "metadata": {
        "id": "6ZbmABqerDhJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterating thru dataset and replacing data\n",
        "for idx, row in dataset.iterrows():\n",
        "    row[0] = row[0].replace('rt', ' ')\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(dataset['text'].values)\n",
        "X = tokenizer.texts_to_sequences(dataset['text'].values)\n",
        "X = pad_sequences(X, maxlen=28)\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "le = LabelEncoder()\n",
        "# fitting and splitting dataset to train and test\n",
        "fitted = le.fit_transform(dataset['sentiment'])\n",
        "y = to_categorical(fitted)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "batch_size = 128\n"
      ],
      "metadata": {
        "id": "QVHjrPAyrByy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Build the Keras model that you have in the PPT use case."
      ],
      "metadata": {
        "id": "l7nSa4-fwHI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating sequential model and adding layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "# compiling model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "tb = TensorBoard(log_dir=\"log/{}\", histogram_freq=0, write_graph=True, write_images=True)\n",
        "model.fit(X_train, Y_train, epochs=5, batch_size=batch_size, verbose=2, callbacks=[tb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxVq44duwLZN",
        "outputId": "87c9481b-7c56-4ab1-ad1b-f0e8db60c2d2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "82/82 - 31s - loss: 0.8824 - accuracy: 0.6190 - 31s/epoch - 373ms/step\n",
            "Epoch 2/5\n",
            "82/82 - 30s - loss: 0.7316 - accuracy: 0.6846 - 30s/epoch - 361ms/step\n",
            "Epoch 3/5\n",
            "82/82 - 28s - loss: 0.6598 - accuracy: 0.7178 - 28s/epoch - 339ms/step\n",
            "Epoch 4/5\n",
            "82/82 - 28s - loss: 0.6195 - accuracy: 0.7381 - 28s/epoch - 340ms/step\n",
            "Epoch 5/5\n",
            "82/82 - 28s - loss: 0.5932 - accuracy: 0.7500 - 28s/epoch - 339ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fabd7a70110>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train and save the model and use the saved model to predict on new text data"
      ],
      "metadata": {
        "id": "qjAEBM2nwNB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/model.h5')\n",
        "m = load_model('/content/model.h5')\n",
        "example = [['A lot of good things are happening. We are respected again throughout the world, and thats a great '\n",
        "         'thing.@realDonaldTrump']]\n",
        "df = pd.DataFrame(example, index=range(0, 1, 1), columns=list('t'))\n",
        "df['t'] = df['t'].apply(lambda x: x.lower())\n",
        "df['t'] = df['t'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "\n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(df['t'].values)\n",
        "X = tokenizer.texts_to_sequences(df['t'].values)\n",
        "X = pad_sequences(X, maxlen=28)\n",
        "\n",
        "output = m.predict(X)\n",
        "print('Output:', output)\n",
        "print(numpy.where(max(output[0])), \":\", (max(output[0])))\n",
        "print(numpy.argmax(output))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxOMed8wweiw",
        "outputId": "29a22cff-3c53-4c8d-ed7d-26aa73da7661"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: [[0.78973055 0.09900098 0.11126847]]\n",
            "(array([0]),) : 0.78973055\n",
            "0\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 28, 128)           256000    \n",
            "                                                                 \n",
            " spatial_dropout1d_4 (Spatia  (None, 28, 128)          0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 196)               254800    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3)                 591       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 511,391\n",
            "Trainable params: 511,391\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Apply the code on spam data set available in the source code (text classification on the spam.csv data set)\n"
      ],
      "metadata": {
        "id": "4fUqSqmaxJRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading spam.csv\n",
        "spam_data = pd.read_csv('/content/spam.csv', encoding='latin-1')\n",
        "spam_data = spam_data[['v2', 'v1']]\n",
        "spam_data['v2'] = spam_data['v2'].apply(lambda x: x.lower())\n",
        "spam_data['v2'] = spam_data['v2'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures)\n",
        "tokenizer.fit_on_texts(spam_data['v2'].values)\n",
        "X = tokenizer.texts_to_sequences(spam_data['v2'].values)\n",
        "X = pad_sequences(X)\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "# creating model\n",
        "def createmodel():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_fatures, embed_dim, input_length=X.shape[1]))\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "# fitting model\n",
        "integer_encoded = le.fit_transform(spam_data['v1'])\n",
        "y = to_categorical(integer_encoded)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "batch_size = 32\n",
        "model = createmodel()\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, verbose=2)\n",
        "score, acc = model.evaluate(X_test, Y_test, verbose=2, batch_size=batch_size)\n",
        "print(model.metrics_names[0], \":\", score)\n",
        "print(model.metrics_names[1], \":\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzdd9svAxMlF",
        "outputId": "dd4965d6-b85b-4a4d-9951-63c6cde9aa06"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "117/117 - 100s - loss: 0.1603 - accuracy: 0.9445 - 100s/epoch - 853ms/step\n",
            "58/58 - 6s - loss: 0.0869 - accuracy: 0.9777 - 6s/epoch - 109ms/step\n",
            "loss : 0.0869411751627922\n",
            "accuracy : 0.9777052998542786\n"
          ]
        }
      ]
    }
  ]
}