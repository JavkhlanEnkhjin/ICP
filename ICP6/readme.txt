Question 1:

a. In order to delete the outliers first we must make a few adjustments and checks. Using train[train.columns[train.isnull().any()]].isnull().sum() we checked the amount of null values found in the train data, which we then removed from the data set using train.select_dtypes(include=[np.number]).interpolate().dropna(). Next we are ready to investigate the outliers, through visual representation and ploting the scatter plot for 'SalePrice' and 'GarageArea' we found out that it is suitable to remove the outliers beyond the 1200 'GarageArea' value and more than 500000 'SalePrice' value. We removed the values using .drop(index_names, inplace=True). Then drew the scatter plot again to check how it looks after the removal of the outliers.

b. 'OverallQual' is a positively correlated feature with 'SalePrice' so we created 2 value variables, one x for 'OverallQual' and another y for 'SalePrice' then used the train_test_split function to split the test data and train data from the main data set and set the test size to be 30%. Next we called the LinearRegression() then fit the X_train on the y_train after that preficted the X_test results and X train result to be used in plotting the regression line between the two features. We evaluated the model using MSE, RMSE, MAE, and R2.

c. We found the top 5 most correlated features to the target label by selecting the numeric features using data.select_dtypes(include=[np.number]) and then correlating them using .corr(). Next we printed the values correlating to SalePrice by descending order to get the highest values at the top. corr['SalePrice'].sort_values(ascending=False)[:]. We discovered the 5 most correlated features were 'OverallQual','GrLivArea','GarageCars','TotalBsmtSF', and 'GarageArea' we split the data set into test and train with test taking 30% similarly to the first model then used StandardScaler() to fit_transform the train data, then transform the test data then used the linear regression followed by fitting and predicting the test data. Finally we evaluated the model through MSE, RMSE, MAE, and R2.

d. We first applied PCA using the PCA function PCA(n_components = 2) by setting the number of componenets to 2 meaning that scikit-learn choose 2 of principal components, next we fit transform the train data then and transform the test data, creating the PCA data frame using principalDf = pd.DataFrame(data =PCAx, columns = ['PCA_1', 'PCA_2']), then fit the x train with the y train followed by finding the prediction of the x test. Finally we evaluated the model through MSE, RMSE, MAE, and R2.

Question 2:

a. for the credit card data set we started by checking the null values and replacing it by the mean using dataset = dataset.fillna(dataset.mean()).

b. to find the elbow method we need to create a for loop that iterated through the range of 1 to 10, and applying kmeans to each cluster using k = KMeans(n_clusters=i, random_state=0). we fit the dataset on each kmean number of cluster and then append it to the klist. When done we use the list to plot the elbow method graph using plt.plot(range(1,10), klist).

c. Next we calculate the silhouette score by first calling the kmeans method KMeans(n_clusters=3, random_state=0) and setting the number of clusters to 3 then fitting the dataset on the kmeans function, followed by predicting the clusters of the dataset and finally calculating the silhouetter score which was 0.4657.

d. Next we tried the scaling feature using the preprocessing.StandardScaler() then fitting the dataset, then transforming the data set to an array which was used to create a new data frame with the same columns of the original dataset using ds = pd.DataFrame(s_array, columns=dataset.columns). We then applied Kmeans similarly to the above question with number of clusters set to 3 and then fit the new dataframe and calculated the silloutte score which was 0.3398 which is not an improvement from the first.

e. finally we applied pca using PCA(n_components = 2) and setting the number of components to 2 then fit transform the dataset which gave us a PCA data set then we initialized the kmean funciton with 3 clusters, then fit the kmean on the PCA data set using .fit(PCAds) and then predicting the number of clusters and finding the silhouette score at the end which was 0.4700 which is an improvement from all the previous models.
